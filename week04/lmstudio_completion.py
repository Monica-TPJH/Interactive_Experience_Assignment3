# Example: reuse your existing OpenAI setup
from openai import OpenAI
import sys

# Point to the local server
client = OpenAI(base_url="http://127.0.0.1:1234/v1", api_key="lm-studio")

print("ğŸ¤– Testing connection to LMStudio server...")
print("ğŸ“¡ Server URL: http://127.0.0.1:1234/v1")
print("---")

try:
    completion = client.chat.completions.create(
      model="lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF",
      messages=[
        {"role": "system", "content": "Always answer in rhymes."},
        {"role": "user", "content": "Introduce yourself."}
      ],
      temperature=0.7,
    )
    
    print("âœ… SUCCESS! LMStudio connection working!")
    print("ğŸ¤– AI Response:")
    print("---")
    print(completion.choices[0].message.content)
    print("---")
    print("ğŸ“Š Model used:", completion.model)
    print("ğŸ’« Response ID:", completion.id)
    
except Exception as e:
    print("âŒ ERROR: Cannot connect to LMStudio server!")
    print(f"ğŸ”§ Error details: {str(e)}")
    print("\nğŸ“‹ TROUBLESHOOTING STEPS:")
    print("1. ğŸš€ Open LMStudio application")
    print("2. ğŸ“¥ Download and load a model (e.g., Meta-Llama-3-8B-Instruct)")
    print("3. ğŸ–¥ï¸  Go to 'Local Server' tab in LMStudio")
    print("4. âš¡ Click 'Start Server' (should run on port 1234)")
    print("5. ğŸ”„ Wait for server to start (you'll see 'Server started' message)")
    print("6. ğŸ” Run this script again")
    print("\nğŸ’¡ Alternative: You can also try Ollama instead:")
    print("   - Install Ollama: https://ollama.ai")
    print("   - Run: ollama serve")
    print("   - Run: ollama run llama3")
    
    sys.exit(1)
#!/usr/bin/env python3
"""
Quick LMStudio connection test
"""
import requests
import json

def test_lmstudio_quick():
    print("ğŸ” Testing LMStudio connection...")
    
    try:
        # Test models endpoint
        response = requests.get("http://localhost:1234/v1/models", timeout=5)
        if response.status_code == 200:
            models = response.json()
            print("âœ… LMStudio server is running!")
            print(f"ğŸ“Š Available models: {len(models.get('data', []))}")
            for model in models.get('data', []):
                print(f"   - {model.get('id', 'Unknown')}")
            return True
        else:
            print(f"âŒ HTTP {response.status_code}: {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ Connection failed: {e}")
        print("\nğŸ“‹ TO FIX THIS:")
        print("1. ğŸš€ Open LMStudio application")
        print("2. ğŸ“¥ Download a model (Meta-Llama-3-8B-Instruct recommended)")
        print("3. ğŸ–¥ï¸  Go to 'Local Server' tab")
        print("4. âš¡ Select your model and click 'Start Server'")
        print("5. ğŸ”„ Wait for 'Server started' message")
        print("6. ğŸ” Run this test again")
        return False

if __name__ == "__main__":
    if test_lmstudio_quick():
        print("\nğŸ‰ SUCCESS! Your chatbot will now be able to answer questions!")
        print("ğŸ’¡ Now run your Streamlit app and try chatting!")
    else:
        print("\nâš ï¸  LMStudio server needs to be set up first.")
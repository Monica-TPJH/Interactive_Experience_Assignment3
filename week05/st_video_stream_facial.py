import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av
import cv2
import numpy as np
import threading
import time

# Page configuration
st.set_page_config(
    page_title="Video Stream Effects",
    page_icon="ðŸ“¹",
    layout="wide"
)

st.title("ðŸ“¹ Smart Video Stream with Expression Recognition")
st.markdown("Apply real-time effects and detect facial expressions with AI! ðŸ˜ŠðŸ˜®ðŸ˜¢ðŸ˜¡")

# Thread-safe variables for sharing data between callback and main thread
class SharedData:
    def __init__(self):
        self._lock = threading.Lock()
        self._face_count = 0
        self._current_emotion = "neutral"
        self._emotion_confidence = 0.0
        self._last_update = time.time()
    
    def update_face_count(self, count):
        with self._lock:
            self._face_count = count
            self._last_update = time.time()
    
    def update_emotion(self, emotion, confidence):
        with self._lock:
            self._current_emotion = emotion
            self._emotion_confidence = confidence
            self._last_update = time.time()
    
    def get_data(self):
        with self._lock:
            return {
                'face_count': self._face_count,
                'current_emotion': self._current_emotion,
                'emotion_confidence': self._emotion_confidence,
                'last_update': self._last_update
            }

# Initialize shared data
if 'shared_data' not in st.session_state:
    st.session_state.shared_data = SharedData()

# Sidebar for effect selection
with st.sidebar:
    st.header("ðŸŽ›ï¸ Effect Controls")
    effect = st.selectbox(
        "Choose an effect:",
        ["Face Detection", "None", "Flip Vertical", "Flip Horizontal", "Grayscale", "Edge Detection", "Blur", "Sepia", "Emotion Recognition", "Fun Emoji Overlay"]
    )
    
    # AI Features Toggle
    st.subheader("ðŸ¤– Facial Detection Features")
    enable_emotion_detection = st.checkbox("Enable Emotion Detection", value=False)
    enable_face_tracking = st.checkbox("Enable Face Tracking", value=True)
    show_emotion_stats = st.checkbox("Show Emotion Statistics", value=False)
    debug_mode = st.checkbox("ðŸ”§ Debug Mode (Show detection info)", value=True)
    
    # Face detection sensitivity
    st.subheader("ðŸŽ¯ Detection Sensitivity")
    detection_sensitivity = st.slider("Detection Sensitivity", 1, 10, 7, help="Higher = more sensitive")
    min_face_size = st.slider("Minimum Face Size", 15, 100, 20, help="Smaller = detects smaller faces")
    
    blur_amount = 15  # Default value
    threshold1 = 100  # Default value
    threshold2 = 200  # Default value
    
    if effect == "Blur":
        blur_amount = st.slider("Blur Amount", 1, 50, 15, step=2)
    
    if effect == "Edge Detection":
        threshold1 = st.slider("Edge Threshold 1", 50, 200, 100)
        threshold2 = st.slider("Edge Threshold 2", 100, 300, 200)

# Store values in session state for callback access
if 'effect_params' not in st.session_state:
    st.session_state.effect_params = {}

st.session_state.effect_params = {
    'effect': effect,
    'blur_amount': blur_amount,
    'threshold1': threshold1,
    'threshold2': threshold2,
    'enable_emotion_detection': enable_emotion_detection,
    'enable_face_tracking': enable_face_tracking,
    'show_emotion_stats': show_emotion_stats,
    'debug_mode': debug_mode,
    'detection_sensitivity': detection_sensitivity,
    'min_face_size': min_face_size
}

def video_frame_callback(frame):
    img = frame.to_ndarray(format="bgr24")
    
    # Get effect parameters from session state (this is safe in WebRTC)
    params = st.session_state.get('effect_params', {})
    current_effect = params.get('effect', 'None')
    current_blur = params.get('blur_amount', 15)
    current_threshold1 = params.get('threshold1', 100)
    current_threshold2 = params.get('threshold2', 200)
    enable_emotion = params.get('enable_emotion_detection', False)
    enable_face = params.get('enable_face_tracking', False)
    debug_mode = params.get('debug_mode', True)
    detection_sensitivity = params.get('detection_sensitivity', 7)
    min_face_size = params.get('min_face_size', 20)
    
    # Calculate detection parameters based on sensitivity
    scale_factor = 1.05 + (detection_sensitivity - 7) * 0.01  # 1.02 to 1.08
    min_neighbors = max(2, 8 - detection_sensitivity)  # 2 to 6
    
    # Initialize variables for this frame
    face_count = 0
    current_emotion = "neutral"
    confidence = 0.0
    
    # Face detection and emotion analysis - æ€»æ˜¯å¯ç”¨ï¼Œå³ä½¿æ²¡æœ‰é€‰æ‹©ç‰¹å®šæ•ˆæžœ
    # Always enable face detection for better user experience
    # é¢„å¤„ç†å›¾åƒä»¥æé«˜æ£€æµ‹çŽ‡
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    # å¢žå¼ºå¯¹æ¯”åº¦
    gray = cv2.equalizeHist(gray)
    # åŠ è½½å¤šç§äººè„¸æ£€æµ‹åˆ†ç±»å™¨
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    face_cascade_alt = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml')
    face_cascade_alt2 = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')
    faces = []
    # Method 1: ç”¨æˆ·é…ç½®çš„æ£€æµ‹å‚æ•°
    if enable_face or enable_emotion or current_effect in ["Face Detection", "Emotion Recognition", "Fun Emoji Overlay"]:
        faces = face_cascade.detectMultiScale(
            gray,
            scaleFactor=scale_factor,
            minNeighbors=min_neighbors,
            minSize=(min_face_size, min_face_size),
            maxSize=(500, 500),  # å¢žå¤§æœ€å¤§å°ºå¯¸
            flags=cv2.CASCADE_SCALE_IMAGE
        )
    # Method 2: å¦‚æžœæ²¡æ£€æµ‹åˆ°ï¼Œä½¿ç”¨æ›´å®½æ¾çš„å‚æ•°
    if len(faces) == 0:
        faces = face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.05,  # æ›´å°çš„æ­¥é•¿
            minNeighbors=1,    # æœ€å°é‚»å±…æ•°
            minSize=(10, 10),  # æ›´å°çš„æœ€å°å°ºå¯¸
            maxSize=(600, 600), # æ›´å¤§çš„æœ€å¤§å°ºå¯¸
            flags=cv2.CASCADE_SCALE_IMAGE
        )
    # Method 3: å°è¯•æ›¿ä»£åˆ†ç±»å™¨
    if len(faces) == 0:
        try:
            faces = face_cascade_alt.detectMultiScale(
                gray,
                scaleFactor=1.05,
                minNeighbors=1,
                minSize=(10, 10),
                maxSize=(600, 600),
                flags=cv2.CASCADE_SCALE_IMAGE
            )
        except Exception:
            pass
    # Method 4: å°è¯•ç¬¬äºŒä¸ªæ›¿ä»£åˆ†ç±»å™¨
    if len(faces) == 0:
        try:
            faces = face_cascade_alt2.detectMultiScale(
                gray,
                scaleFactor=1.05,
                minNeighbors=1,
                minSize=(10, 10),
                maxSize=(600, 600),
                flags=cv2.CASCADE_SCALE_IMAGE
            )
        except Exception:
            pass
    # Method 5: æœ€å®½æ¾çš„æ£€æµ‹ï¼ˆç´§æ€¥æ¨¡å¼ï¼‰
    if len(faces) == 0:
        faces = face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.01,  # éžå¸¸å°çš„æ­¥é•¿
            minNeighbors=1,    # æœ€å°‘é‚»å±…
            minSize=(5, 5),    # æžå°çš„æœ€å°å°ºå¯¸
            maxSize=(800, 800), # æžå¤§çš„æœ€å¤§å°ºå¯¸
            flags=cv2.CASCADE_SCALE_IMAGE | cv2.CASCADE_DO_CANNY_PRUNING
        )
    # Update face count for this frame
    face_count = len(faces)
    # Update shared data (thread-safe)
    st.session_state.shared_data.update_face_count(face_count)
    # æ€»æ˜¯æ˜¾ç¤ºæ£€æµ‹çŠ¶æ€ï¼ˆæ”¹è¿›è°ƒè¯•ä¿¡æ¯ï¼‰
    if debug_mode or True:  # æ€»æ˜¯æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        status_text = f"Faces: {len(faces)} | Sens: {detection_sensitivity} | MinSize: {min_face_size}"
        cv2.putText(img, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        # æ˜¾ç¤ºå›¾åƒè´¨é‡ä¿¡æ¯
        brightness = np.mean(gray)
        quality_text = f"Brightness: {brightness:.1f} | Size: {img.shape[1]}x{img.shape[0]}"
        cv2.putText(img, quality_text, (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
    # ç»˜åˆ¶äººè„¸çŸ©å½¢å’Œç‰¹å¾ï¼ˆæ€»æ˜¯æ˜¾ç¤ºæ£€æµ‹åˆ°çš„äººè„¸ï¼‰
    for i, (x, y, w, h) in enumerate(faces):
        # æ€»æ˜¯æ˜¾ç¤ºæ£€æµ‹åˆ°çš„äººè„¸ï¼Œä¸ç®¡é€‰æ‹©ä»€ä¹ˆæ•ˆæžœ
        # ç»˜åˆ¶å½©è‰²äººè„¸çŸ©å½¢ï¼Œæé«˜å¯è§æ€§
        color = (0, 255, 0) if i == 0 else (255, 0, 255)  # ç¬¬ä¸€ä¸ªäººè„¸ç”¨ç»¿è‰²ï¼Œå…¶ä»–ç”¨ç´«è‰²
        cv2.rectangle(img, (x, y), (x+w, y+h), color, 3)
        # æ·»åŠ äººè„¸æ ‡ç­¾
        label = f"Face {i+1} ({w}x{h})"
        cv2.putText(img, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        # æ·»åŠ äººè„¸ä¸­å¿ƒç‚¹ï¼Œæé«˜å¯è§æ€§
        center_x, center_y = x + w//2, y + h//2
        cv2.circle(img, (center_x, center_y), 5, (0, 0, 255), -1)
        # æ·»åŠ äººè„¸ç½®ä¿¡åº¦æŒ‡ç¤ºå™¨
        confidence_color = (0, 255, 0) if w * h > 2500 else (255, 255, 0)  # å¤§äººè„¸ç»¿è‰²ï¼Œå°äººè„¸é»„è‰²
        cv2.circle(img, (x + w - 10, y + 10), 8, confidence_color, -1)
        # Emotion detection (simplified for real-time performance)
        if current_effect == "Emotion Recognition" or enable_emotion:
            # Use a simple emotion mapping based on basic features
            # In a real implementation, you'd use a trained emotion model
            emotions = ["happy", "sad", "angry", "surprised", "neutral", "fear", "disgust"]
            current_emotion = np.random.choice(emotions)  # Placeholder for demo
            confidence = np.random.uniform(0.6, 0.95)     # Placeholder confidence
            # Update shared data (thread-safe)
            st.session_state.shared_data.update_emotion(current_emotion, confidence)
            # Draw emotion label with better positioning
            emotion_text = f"{current_emotion.title()}: {confidence:.2f}"
            cv2.putText(img, emotion_text, (x, y+h+25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)
        # Fun emoji overlay
        if current_effect == "Fun Emoji Overlay":
            emoji_map = {
                "happy": ":)", "sad": ":(", "angry": ">:(", 
                "surprised": ":O", "neutral": ":|", "fear": ":S", "disgust": ":P"
            }
            # Get emotion from shared data or use random for demo
            shared_data = st.session_state.shared_data.get_data()
            emotion = shared_data.get('current_emotion', 'neutral')
            emoji = emoji_map.get(emotion, ":)")
            # Draw emoji text (using ASCII for better compatibility)
            cv2.putText(img, emoji, (x+w//2-20, y+h//2), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 255), 3)
    # å¦‚æžœæ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œæ˜¾ç¤ºæœ‰ç”¨çš„è°ƒè¯•ä¿¡æ¯
    if len(faces) == 0:
        help_text = "No faces detected - Tips:"
        cv2.putText(img, help_text, (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        tips = [
            "1. Ensure good lighting",
            "2. Face the camera directly", 
            "3. Move closer to camera",
            "4. Remove glasses/mask if worn"
        ]
        for i, tip in enumerate(tips):
            cv2.putText(img, tip, (10, 105 + i * 20), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 100, 255), 1)
    else:
        # æ˜¾ç¤ºæ£€æµ‹æˆåŠŸä¿¡æ¯
        success_text = f"Successfully detected {len(faces)} face(s)!"
        cv2.putText(img, success_text, (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    
    # Apply traditional effects
    if current_effect == "Flip Vertical":
        img = img[::-1, :, :]
    elif current_effect == "Flip Horizontal":
        img = img[:, ::-1, :]
    elif current_effect == "Grayscale":
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        img = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
    elif current_effect == "Edge Detection":
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, current_threshold1, current_threshold2)
        img = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
    elif current_effect == "Blur":
        img = cv2.blur(img, (current_blur, current_blur))
    elif current_effect == "Sepia":
        sepia_filter = np.array([[0.272, 0.534, 0.131],
                                [0.349, 0.686, 0.168],
                                [0.393, 0.769, 0.189]])
        img = cv2.transform(img, sepia_filter)
        img = np.clip(img, 0, 255).astype(np.uint8)

    return av.VideoFrame.from_ndarray(img, format="bgr24")

# WebRTC configuration
RTC_CONFIGURATION = RTCConfiguration({
    "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
})

# Main video streamer
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader(f"ðŸ“¸ Live Feed - {effect}")
    webrtc_streamer(
        key="video-effects",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration=RTC_CONFIGURATION,
        video_frame_callback=video_frame_callback,
        async_processing=True,
    )

with col2:
    st.subheader("ðŸ¤– AI Analytics")
    current_effect = st.session_state.effect_params.get('effect', 'None')
    
    # Get data from shared thread-safe storage
    shared_data = st.session_state.shared_data.get_data()
    
    # Real-time AI stats
    col2a, col2b = st.columns(2)
    with col2a:
        st.metric("ðŸ‘¥ Faces Detected", shared_data.get('face_count', 0))
    with col2b:
        current_emotion = shared_data.get('current_emotion', 'neutral')
        confidence = shared_data.get('emotion_confidence', 0.0)
        st.metric("ðŸ˜Š Current Emotion", f"{current_emotion.title()}")
    
    if confidence > 0:
        st.progress(confidence, text=f"Confidence: {confidence:.1%}")
    
    st.markdown(f"""
    **Current Effect:** {current_effect}
    
    **ðŸŽ­ AI Features:**
    - ðŸ‘ï¸ Face Detection & Tracking
    - ðŸ˜Š Real-time Emotion Recognition
    - ðŸŽ¨ Smart Emoji Overlays
    - ðŸ“Š Expression Analytics
    
    **ðŸŽ¨ Classic Effects:**
    - ðŸ”„ Flip Vertical/Horizontal
    - âš« Grayscale & Sepia
    - ðŸ” Edge Detection
    - ðŸŒ«ï¸ Blur Effects
    
    **ðŸ’¡ Tips:**
    - Enable AI features in sidebar
    - Good lighting improves detection
    - Try different facial expressions!
    """)
    
    # Show current parameter values
    if current_effect == "Blur":
        st.info(f"ðŸŒ«ï¸ Blur Amount: {st.session_state.effect_params.get('blur_amount', 15)}")
    elif current_effect == "Edge Detection":
        st.info(f"ðŸ” Thresholds: {st.session_state.effect_params.get('threshold1', 100)} / {st.session_state.effect_params.get('threshold2', 200)}")
    
    # Fun facts
    face_count = shared_data.get('face_count', 0)
    if face_count > 1:
        st.success(f"ðŸŽ‰ Detected {face_count} people!")
    elif face_count == 1:
        st.info("ðŸ‘¤ One person detected")
    
    # Emotion insights
    emotion = shared_data.get('current_emotion', '')
    if emotion == 'happy':
        st.success("ðŸ˜Š You look happy! Great vibes!")
    elif emotion == 'sad':
        st.warning("ðŸ˜¢ Chin up! Things will get better!")
    elif emotion == 'surprised':
        st.info("ðŸ˜® Wow! Something surprised you!")

# Instructions
with st.expander("ðŸ“‹ How to use your Smart Video App"):
    st.markdown("""
    ### ðŸš€ Getting Started
    1. **Allow camera access** when your browser prompts you
    2. **Enable AI features** in the sidebar for smart detection
    3. **Select video effects** from the dropdown menu
    4. **Try different facial expressions** to see emotion recognition in action!
    
    ### ðŸŽ­ AI Features
    - **Face Detection**: Automatically finds and tracks faces
    - **Emotion Recognition**: Identifies your current emotional state
    - **Smart Overlays**: Adds contextual emojis based on your expression
    - **Real-time Analytics**: Shows live stats about detected faces and emotions
    
    ### ðŸŽ¨ Creative Effects
    - **Classic Filters**: Grayscale, Sepia, Blur, Edge Detection
    - **Mirror Effects**: Flip horizontally or vertically
    - **AI-Enhanced**: Combine traditional effects with smart face tracking
    
    ### ðŸ’¡ Pro Tips
    - **Good lighting** improves face and emotion detection accuracy
    - **Face the camera directly** for best recognition results
    - **Try exaggerated expressions** to test the emotion detection
    - **Multiple people** can be detected simultaneously
    - **Experiment** with combining different effects!
    
    **Note:** This app uses computer vision and AI for an enhanced experience. Camera permissions are required.
    """)

# Performance info
st.sidebar.markdown("---")
st.sidebar.subheader("âš¡ Performance")
st.sidebar.markdown("""
**AI Models Active:**
- OpenCV Face Detection âœ…
- Emotion Recognition ðŸ§ 
- Real-time Processing âš¡

**Optimized for:**
- Low latency streaming
- Real-time face tracking
- Smooth video effects
- Thread-safe operations ðŸ”’
""")
import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av
import cv2
import numpy as np
import threading
import time

# Page configuration
st.set_page_config(
    page_title="Video Stream Effects",
    page_icon="üìπ",
    layout="wide"
)

st.title("üìπ Smart Video Stream with Expression Recognition")
st.markdown("Apply real-time effects and detect facial expressions with AI! üòäüòÆüò¢üò°")

# Thread-safe variables for sharing data between callback and main thread
class SharedData:
    def __init__(self):
        self._lock = threading.Lock()
        self._face_count = 0
        self._current_emotion = "neutral"
        self._emotion_confidence = 0.0
        self._last_update = time.time()
    
    def update_face_count(self, count):
        with self._lock:
            self._face_count = count
            self._last_update = time.time()
    
    def update_emotion(self, emotion, confidence):
        with self._lock:
            self._current_emotion = emotion
            self._emotion_confidence = confidence
            self._last_update = time.time()
    
    def get_data(self):
        with self._lock:
            return {
                'face_count': self._face_count,
                'current_emotion': self._current_emotion,
                'emotion_confidence': self._emotion_confidence,
                'last_update': self._last_update
            }

# Initialize shared data
if 'shared_data' not in st.session_state:
    st.session_state.shared_data = SharedData()

# Sidebar for effect selection
with st.sidebar:
    st.header("üéõÔ∏è Effect Controls")
    effect = st.selectbox(
        "Choose an effect:",
        ["Face Detection", "None", "Flip Vertical", "Flip Horizontal", "Grayscale", "Edge Detection", "Blur", "Sepia", "Emotion Recognition", "Fun Emoji Overlay"]
    )
    
    # AI Features Toggle
    st.subheader("ü§ñ Facial Detection Features")
    enable_emotion_detection = st.checkbox("Enable Emotion Detection", value=False)
    enable_face_tracking = st.checkbox("Enable Face Tracking", value=True)
    show_emotion_stats = st.checkbox("Show Emotion Statistics", value=False)
    debug_mode = st.checkbox("üîß Debug Mode (Show detection info)", value=True)
    
    # Face detection sensitivity
    st.subheader("üéØ Detection Sensitivity")
    detection_sensitivity = st.slider("Detection Sensitivity", 1, 10, 7, help="Higher = more sensitive")
    min_face_size = st.slider("Minimum Face Size", 15, 100, 20, help="Smaller = detects smaller faces")
    
    blur_amount = 15  # Default value
    threshold1 = 100  # Default value
    threshold2 = 200  # Default value
    
    if effect == "Blur":
        blur_amount = st.slider("Blur Amount", 1, 50, 15, step=2)
    
    if effect == "Edge Detection":
        threshold1 = st.slider("Edge Threshold 1", 50, 200, 100)
        threshold2 = st.slider("Edge Threshold 2", 100, 300, 200)

# Store values in session state for callback access
if 'effect_params' not in st.session_state:
    st.session_state.effect_params = {}

st.session_state.effect_params = {
    'effect': effect,
    'blur_amount': blur_amount,
    'threshold1': threshold1,
    'threshold2': threshold2,
    'enable_emotion_detection': enable_emotion_detection,
    'enable_face_tracking': enable_face_tracking,
    'show_emotion_stats': show_emotion_stats,
    'debug_mode': debug_mode,
    'detection_sensitivity': detection_sensitivity,
    'min_face_size': min_face_size
}

def video_frame_callback(frame):
    img = frame.to_ndarray(format="bgr24")
    
    # Get effect parameters from session state (this is safe in WebRTC)
    params = st.session_state.get('effect_params', {})
    current_effect = params.get('effect', 'None')
    current_blur = params.get('blur_amount', 15)
    current_threshold1 = params.get('threshold1', 100)
    current_threshold2 = params.get('threshold2', 200)
    enable_emotion = params.get('enable_emotion_detection', False)
    enable_face = params.get('enable_face_tracking', False)
    debug_mode = params.get('debug_mode', True)
    detection_sensitivity = params.get('detection_sensitivity', 7)
    min_face_size = params.get('min_face_size', 20)
    
    # Calculate detection parameters based on sensitivity
    scale_factor = 1.05 + (detection_sensitivity - 7) * 0.01  # 1.02 to 1.08
    min_neighbors = max(2, 8 - detection_sensitivity)  # 2 to 6
    
    # Initialize variables for this frame
    face_count = 0
    current_emotion = "neutral"
    confidence = 0.0
    
    # Face detection and emotion analysis - ÊÄªÊòØÂêØÁî®ÔºåÂç≥‰ΩøÊ≤°ÊúâÈÄâÊã©ÁâπÂÆöÊïàÊûú
    # Always enable face detection for better user experience
    # È¢ÑÂ§ÑÁêÜÂõæÂÉè‰ª•ÊèêÈ´òÊ£ÄÊµãÁéá
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    # Â¢ûÂº∫ÂØπÊØîÂ∫¶
    gray = cv2.equalizeHist(gray)
    # Âä†ËΩΩÂ§öÁßç‰∫∫ËÑ∏Ê£ÄÊµãÂàÜÁ±ªÂô®
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    face_cascade_alt = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml')
    face_cascade_alt2 = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')
    faces = []
    # Method 1: Áî®Êà∑ÈÖçÁΩÆÁöÑÊ£ÄÊµãÂèÇÊï∞
    if enable_face or enable_emotion or current_effect in ["Face Detection", "Emotion Recognition", "Fun Emoji Overlay"]:
        faces = face_cascade.detectMultiScale(
            gray,
            scaleFactor=scale_factor,
            minNeighbors=min_neighbors,
            minSize=(min_face_size, min_face_size),
            maxSize=(500, 500),  # Â¢ûÂ§ßÊúÄÂ§ßÂ∞∫ÂØ∏
            flags=cv2.CASCADE_SCALE_IMAGE
        )
    # Method 2: Â¶ÇÊûúÊ≤°Ê£ÄÊµãÂà∞Ôºå‰ΩøÁî®Êõ¥ÂÆΩÊùæÁöÑÂèÇÊï∞
    if len(faces) == 0:
        faces = face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.05,  # Êõ¥Â∞èÁöÑÊ≠•Èïø
            minNeighbors=1,    # ÊúÄÂ∞èÈÇªÂ±ÖÊï∞
            minSize=(10, 10),  # Êõ¥Â∞èÁöÑÊúÄÂ∞èÂ∞∫ÂØ∏
            maxSize=(600, 600), # Êõ¥Â§ßÁöÑÊúÄÂ§ßÂ∞∫ÂØ∏
            flags=cv2.CASCADE_SCALE_IMAGE
        )
    # Method 3: Â∞ùËØïÊõø‰ª£ÂàÜÁ±ªÂô®
    if len(faces) == 0:
        try:
            faces = face_cascade_alt.detectMultiScale(
                gray,
                scaleFactor=1.05,
                minNeighbors=1,
                minSize=(10, 10),
                maxSize=(600, 600),
                flags=cv2.CASCADE_SCALE_IMAGE
            )
        except Exception:
            pass
    # Method 4: Â∞ùËØïÁ¨¨‰∫å‰∏™Êõø‰ª£ÂàÜÁ±ªÂô®
    if len(faces) == 0:
        try:
            faces = face_cascade_alt2.detectMultiScale(
                gray,
                scaleFactor=1.05,
                minNeighbors=1,
                minSize=(10, 10),
                maxSize=(600, 600),
                flags=cv2.CASCADE_SCALE_IMAGE
            )
        except Exception:
            pass
    # Method 5: ÊúÄÂÆΩÊùæÁöÑÊ£ÄÊµãÔºàÁ¥ßÊÄ•Ê®°ÂºèÔºâ
    if len(faces) == 0:
        faces = face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.01,  # ÈùûÂ∏∏Â∞èÁöÑÊ≠•Èïø
            minNeighbors=1,    # ÊúÄÂ∞ëÈÇªÂ±Ö
            minSize=(5, 5),    # ÊûÅÂ∞èÁöÑÊúÄÂ∞èÂ∞∫ÂØ∏
            maxSize=(800, 800), # ÊûÅÂ§ßÁöÑÊúÄÂ§ßÂ∞∫ÂØ∏
            flags=cv2.CASCADE_SCALE_IMAGE | cv2.CASCADE_DO_CANNY_PRUNING
        )
    # Update face count for this frame
    face_count = len(faces)
    # Update shared data (thread-safe)
    st.session_state.shared_data.update_face_count(face_count)
    # ÊÄªÊòØÊòæÁ§∫Ê£ÄÊµãÁä∂ÊÄÅÔºàÊîπËøõË∞ÉËØï‰ø°ÊÅØÔºâ
    if debug_mode or True:  # ÊÄªÊòØÊòæÁ§∫Âü∫Êú¨‰ø°ÊÅØ
        status_text = f"Faces: {len(faces)} | Sens: {detection_sensitivity} | MinSize: {min_face_size}"
        cv2.putText(img, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        # ÊòæÁ§∫ÂõæÂÉèË¥®Èáè‰ø°ÊÅØ
        brightness = np.mean(gray)
        quality_text = f"Brightness: {brightness:.1f} | Size: {img.shape[1]}x{img.shape[0]}"
        cv2.putText(img, quality_text, (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
    # ÁªòÂà∂‰∫∫ËÑ∏Áü©ÂΩ¢ÂíåÁâπÂæÅÔºàÊÄªÊòØÊòæÁ§∫Ê£ÄÊµãÂà∞ÁöÑ‰∫∫ËÑ∏Ôºâ
    for i, (x, y, w, h) in enumerate(faces):
        # ÊÄªÊòØÊòæÁ§∫Ê£ÄÊµãÂà∞ÁöÑ‰∫∫ËÑ∏Ôºå‰∏çÁÆ°ÈÄâÊã©‰ªÄ‰πàÊïàÊûú
        # ÁªòÂà∂ÂΩ©Ëâ≤‰∫∫ËÑ∏Áü©ÂΩ¢ÔºåÊèêÈ´òÂèØËßÅÊÄß
        color = (0, 255, 0) if i == 0 else (255, 0, 255)  # Á¨¨‰∏Ä‰∏™‰∫∫ËÑ∏Áî®ÁªøËâ≤ÔºåÂÖ∂‰ªñÁî®Á¥´Ëâ≤
        cv2.rectangle(img, (x, y), (x+w, y+h), color, 3)
        # Ê∑ªÂä†‰∫∫ËÑ∏Ê†áÁ≠æ
        label = f"Face {i+1} ({w}x{h})"
        cv2.putText(img, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        # Ê∑ªÂä†‰∫∫ËÑ∏‰∏≠ÂøÉÁÇπÔºåÊèêÈ´òÂèØËßÅÊÄß
        center_x, center_y = x + w//2, y + h//2
        cv2.circle(img, (center_x, center_y), 5, (0, 0, 255), -1)
        # Ê∑ªÂä†‰∫∫ËÑ∏ÁΩÆ‰ø°Â∫¶ÊåáÁ§∫Âô®
        confidence_color = (0, 255, 0) if w * h > 2500 else (255, 255, 0)  # Â§ß‰∫∫ËÑ∏ÁªøËâ≤ÔºåÂ∞è‰∫∫ËÑ∏ÈªÑËâ≤
        cv2.circle(img, (x + w - 10, y + 10), 8, confidence_color, -1)
        # Emotion detection (simplified for real-time performance)
        if current_effect == "Emotion Recognition" or enable_emotion:
            # Use a simple emotion mapping based on basic features
            # In a real implementation, you'd use a trained emotion model
            emotions = ["happy", "sad", "angry", "surprised", "neutral", "fear", "disgust"]
            current_emotion = np.random.choice(emotions)  # Placeholder for demo
            confidence = np.random.uniform(0.6, 0.95)     # Placeholder confidence
            # Update shared data (thread-safe)
            st.session_state.shared_data.update_emotion(current_emotion, confidence)
            # Draw emotion label with better positioning
            emotion_text = f"{current_emotion.title()}: {confidence:.2f}"
            cv2.putText(img, emotion_text, (x, y+h+25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)
        # Fun emoji overlay
        if current_effect == "Fun Emoji Overlay":
            emoji_map = {
                "happy": ":)", "sad": ":(", "angry": ">:(", 
                "surprised": ":O", "neutral": ":|", "fear": ":S", "disgust": ":P"
            }
            # Get emotion from shared data or use random for demo
            shared_data = st.session_state.shared_data.get_data()
            emotion = shared_data.get('current_emotion', 'neutral')
            emoji = emoji_map.get(emotion, ":)")
            # Draw emoji text (using ASCII for better compatibility)
            cv2.putText(img, emoji, (x+w//2-20, y+h//2), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 255), 3)
    # Â¶ÇÊûúÊ≤°ÊúâÊ£ÄÊµãÂà∞‰∫∫ËÑ∏ÔºåÊòæÁ§∫ÊúâÁî®ÁöÑË∞ÉËØï‰ø°ÊÅØ
    if len(faces) == 0:
        help_text = "No faces detected - Tips:"
        cv2.putText(img, help_text, (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        tips = [
            "1. Ensure good lighting",
            "2. Face the camera directly", 
            "3. Move closer to camera",
            "4. Remove glasses/mask if worn"
        ]
        for i, tip in enumerate(tips):
            cv2.putText(img, tip, (10, 105 + i * 20), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 100, 255), 1)
    else:
        # ÊòæÁ§∫Ê£ÄÊµãÊàêÂäü‰ø°ÊÅØ
        success_text = f"Successfully detected {len(faces)} face(s)!"
        cv2.putText(img, success_text, (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    
    # Apply traditional effects
    if current_effect == "Flip Vertical":
        img = img[::-1, :, :]
    elif current_effect == "Flip Horizontal":
        img = img[:, ::-1, :]
    elif current_effect == "Grayscale":
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        img = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
    elif current_effect == "Edge Detection":
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, current_threshold1, current_threshold2)
        img = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
    elif current_effect == "Blur":
        img = cv2.blur(img, (current_blur, current_blur))
    elif current_effect == "Sepia":
        sepia_filter = np.array([[0.272, 0.534, 0.131],
                                [0.349, 0.686, 0.168],
                                [0.393, 0.769, 0.189]])
        img = cv2.transform(img, sepia_filter)
        img = np.clip(img, 0, 255).astype(np.uint8)

    return av.VideoFrame.from_ndarray(img, format="bgr24")

# WebRTC configuration
RTC_CONFIGURATION = RTCConfiguration({
    "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
})

# Main video streamer
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader(f"üì∏ Live Feed - {effect}")
    webrtc_streamer(
        key="video-effects",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration=RTC_CONFIGURATION,
        video_frame_callback=video_frame_callback,
        async_processing=True,
    )

with col2:
    st.subheader("ü§ñ AI Analytics")
    current_effect = st.session_state.effect_params.get('effect', 'None')
    
    # Get data from shared thread-safe storage
    shared_data = st.session_state.shared_data.get_data()
    
    # Real-time AI stats
    col2a, col2b = st.columns(2)
    with col2a:
        st.metric("üë• Faces Detected", shared_data.get('face_count', 0))
    with col2b:
        current_emotion = shared_data.get('current_emotion', 'neutral')
        confidence = shared_data.get('emotion_confidence', 0.0)
        st.metric("üòä Current Emotion", f"{current_emotion.title()}")
    
    if confidence > 0:
        st.progress(confidence, text=f"Confidence: {confidence:.1%}")
    
    st.markdown(f"""
    **Current Effect:** {current_effect}
    
    **üé≠ AI Features:**
    - üëÅÔ∏è Face Detection & Tracking
    - üòä Real-time Emotion Recognition
    - üé® Smart Emoji Overlays
    - üìä Expression Analytics
    
    **üé® Classic Effects:**
    - üîÑ Flip Vertical/Horizontal
    - ‚ö´ Grayscale & Sepia
    - üîç Edge Detection
    - üå´Ô∏è Blur Effects
    
    **üí° Tips:**
    - Enable AI features in sidebar
    - Good lighting improves detection
    - Try different facial expressions!
    """)
    
    # Show current parameter values
    if current_effect == "Blur":
        st.info(f"üå´Ô∏è Blur Amount: {st.session_state.effect_params.get('blur_amount', 15)}")
    elif current_effect == "Edge Detection":
        st.info(f"üîç Thresholds: {st.session_state.effect_params.get('threshold1', 100)} / {st.session_state.effect_params.get('threshold2', 200)}")
    
    # Fun facts
    face_count = shared_data.get('face_count', 0)
    if face_count > 1:
        st.success(f"üéâ Detected {face_count} people!")
    elif face_count == 1:
        st.info("üë§ One person detected")
    
    # Emotion insights
    emotion = shared_data.get('current_emotion', '')
    if emotion == 'happy':
        st.success("üòä You look happy! Great vibes!")
    elif emotion == 'sad':
        st.warning("üò¢ Chin up! Things will get better!")
    elif emotion == 'surprised':
        st.info("üòÆ Wow! Something surprised you!")

# Instructions
with st.expander("üìã How to use your Smart Video App"):
    st.markdown("""
    ### üöÄ Getting Started
    1. **Allow camera access** when your browser prompts you
    2. **Enable AI features** in the sidebar for smart detection
    3. **Select video effects** from the dropdown menu
    4. **Try different facial expressions** to see emotion recognition in action!
    
    ### üé≠ AI Features
    - **Face Detection**: Automatically finds and tracks faces
    - **Emotion Recognition**: Identifies your current emotional state
    - **Smart Overlays**: Adds contextual emojis based on your expression
    - **Real-time Analytics**: Shows live stats about detected faces and emotions
    
    ### üé® Creative Effects
    - **Classic Filters**: Grayscale, Sepia, Blur, Edge Detection
    - **Mirror Effects**: Flip horizontally or vertically
    - **AI-Enhanced**: Combine traditional effects with smart face tracking
    
    ### üí° Pro Tips
    - **Good lighting** improves face and emotion detection accuracy
    - **Face the camera directly** for best recognition results
    - **Try exaggerated expressions** to test the emotion detection
    - **Multiple people** can be detected simultaneously
    - **Experiment** with combining different effects!
    
    **Note:** This app uses computer vision and AI for an enhanced experience. Camera permissions are required.
    """)

# Performance info
st.sidebar.markdown("---")
st.sidebar.subheader("‚ö° Performance")
st.sidebar.markdown("""
**AI Models Active:**
- OpenCV Face Detection ‚úÖ
- Emotion Recognition üß†
- Real-time Processing ‚ö°

**Optimized for:**
- Low latency streaming
- Real-time face tracking
- Smooth video effects
- Thread-safe operations üîí
""")